{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from string import digits\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\OB\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\OB\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\OB\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using GeForce RTX 3080 GPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == torch.device('cpu'):\n",
    "    print('Using cpu')\n",
    "else:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print('Using {} GPUs'.format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                     text  toxic\n0       Explanation\\nWhy the edits made under my usern...      0\n1       D'aww! He matches this background colour I'm s...      0\n2       Hey man, I'm really not trying to edit war. It...      0\n3       \"\\nMore\\nI can't make any real suggestions on ...      0\n4       You, sir, are my hero. Any chance you remember...      0\n...                                                   ...    ...\n159566  \":::::And for the second time of asking, when ...      0\n159567  You should be ashamed of yourself \\n\\nThat is ...      0\n159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n159569  And it looks like it was actually you who put ...      0\n159570  \"\\nAnd ... I really don't think you understand...      0\n\n[159571 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>\":::::And for the second time of asking, when ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>\"\\nAnd ... I really don't think you understand...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_open = pd.read_csv('C:/Users/OB/Desktop/projects_to_do/datasets/7_toxic_comments.csv')\n",
    "display(df_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "def remove_numbers(text):\n",
    "    return text.translate(str.maketrans('', '', digits))\n",
    "def known_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contraction_mapping:\n",
    "            text = text.replace(word, contraction_mapping[word.lower()])\n",
    "    return text\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                     text  toxic\n0       explanation edits made username hardcore metal...      0\n1       daww matches background colour seemingly stuck...      0\n2       hey man really trying edit war guy constantly ...      0\n3       cannot make real suggestions improvement wonde...      0\n4                           sir hero chance remember page      0\n...                                                   ...    ...\n159566  second time asking view completely contradicts...      0\n159567               ashamed horrible thing put talk page      0\n159568  spitzer umm theres actual article prostitution...      0\n159569  looks like actually put speedy first version d...      0\n159570  really think understand came idea bad right aw...      0\n\n[159571 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>explanation edits made username hardcore metal...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>daww matches background colour seemingly stuck...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hey man really trying edit war guy constantly ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cannot make real suggestions improvement wonde...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sir hero chance remember page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>second time asking view completely contradicts...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ashamed horrible thing put talk page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>spitzer umm theres actual article prostitution...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>looks like actually put speedy first version d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>really think understand came idea bad right aw...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 8.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = df_open.copy()\n",
    "df['text']=df['text'].astype(str) #fix the format\n",
    "df['text']=df['text'].str.lower() #lower the strings\n",
    "df['text']=df['text'].apply(lambda text: remove_numbers(text)) #delete numbers\n",
    "df['text']=df['text'].apply(lambda text: known_contractions(text)) #check the grammar\n",
    "df['text']=df['text'].replace('https?:\\/\\/.*\\/\\w*', '', regex=True) #delete hyperlinks\n",
    "df['text']=df['text'].replace('#', ' ', regex=True) #delete hashtags\n",
    "df['text']=df['text'].replace('\\@\\w*', '', regex=True) #delete quotes\n",
    "df['text']=df['text'].replace('\\$\\w*', '', regex=True) #delete tickers\n",
    "df['text']=df['text'].apply(lambda text: remove_punctuation(text)) #delete punktuation\n",
    "df['text']=df['text'].apply(lambda text: remove_stopwords(text)) #delete stopwords\n",
    "df['text']=df['text'].replace('\\&*[amp]*\\;|gt+', '', regex=True) #delete quotes\n",
    "df['text']=df['text'].replace('\\s+rt\\s+', '', regex=True) #delete RT\n",
    "df['text']=df['text'].replace('[\\n\\t\\r]+', ' ', regex=True) #delete linebreak, tab, return\n",
    "df['text']=df['text'].replace('via+\\s', '', regex=True) #delete via\n",
    "df['text']=df['text'].replace('\\s+\\s+', ' ', regex=True) #delete hyperspaces\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                     text  toxic  \\\n0       explanation edits made username hardcore metal...      0   \n1       daww matches background colour seemingly stuck...      0   \n2       hey man really trying edit war guy constantly ...      0   \n3       cannot make real suggestions improvement wonde...      0   \n4                           sir hero chance remember page      0   \n...                                                   ...    ...   \n159566  second time asking view completely contradicts...      0   \n159567               ashamed horrible thing put talk page      0   \n159568  spitzer umm theres actual article prostitution...      0   \n159569  looks like actually put speedy first version d...      0   \n159570  really think understand came idea bad right aw...      0   \n\n                                               text_stems  \\\n0       explan edit made usernam hardcor metallica fan...   \n1       daww match background colour seem stuck thank ...   \n2       hey man realli tri edit war guy constant remov...   \n3       cannot make real suggest improv wonder section...   \n4                              sir hero chanc rememb page   \n...                                                   ...   \n159566  second time ask view complet contradict covera...   \n159567                  asham horribl thing put talk page   \n159568  spitzer umm there actual articl prostitut ring...   \n159569  look like actual put speedi first version dele...   \n159570  realli think understand came idea bad right aw...   \n\n                                              text_lemmas  \n0       explanation edits make username hardcore metal...  \n1       daww match background colour seemingly stuck t...  \n2       hey man really try edit war guy constantly rem...  \n3       cannot make real suggestion improvement wonder...  \n4                           sir hero chance remember page  \n...                                                   ...  \n159566  second time ask view completely contradict cov...  \n159567               ashamed horrible thing put talk page  \n159568  spitzer umm there actual article prostitution ...  \n159569  look like actually put speedy first version de...  \n159570  really think understand come idea bad right aw...  \n\n[159571 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n      <th>text_stems</th>\n      <th>text_lemmas</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>explanation edits made username hardcore metal...</td>\n      <td>0</td>\n      <td>explan edit made usernam hardcor metallica fan...</td>\n      <td>explanation edits make username hardcore metal...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>daww matches background colour seemingly stuck...</td>\n      <td>0</td>\n      <td>daww match background colour seem stuck thank ...</td>\n      <td>daww match background colour seemingly stuck t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hey man really trying edit war guy constantly ...</td>\n      <td>0</td>\n      <td>hey man realli tri edit war guy constant remov...</td>\n      <td>hey man really try edit war guy constantly rem...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cannot make real suggestions improvement wonde...</td>\n      <td>0</td>\n      <td>cannot make real suggest improv wonder section...</td>\n      <td>cannot make real suggestion improvement wonder...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sir hero chance remember page</td>\n      <td>0</td>\n      <td>sir hero chanc rememb page</td>\n      <td>sir hero chance remember page</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>second time asking view completely contradicts...</td>\n      <td>0</td>\n      <td>second time ask view complet contradict covera...</td>\n      <td>second time ask view completely contradict cov...</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ashamed horrible thing put talk page</td>\n      <td>0</td>\n      <td>asham horribl thing put talk page</td>\n      <td>ashamed horrible thing put talk page</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>spitzer umm theres actual article prostitution...</td>\n      <td>0</td>\n      <td>spitzer umm there actual articl prostitut ring...</td>\n      <td>spitzer umm there actual article prostitution ...</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>looks like actually put speedy first version d...</td>\n      <td>0</td>\n      <td>look like actual put speedi first version dele...</td>\n      <td>look like actually put speedy first version de...</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>really think understand came idea bad right aw...</td>\n      <td>0</td>\n      <td>realli think understand came idea bad right aw...</td>\n      <td>really think understand come idea bad right aw...</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 4 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text_stems'] = df['text'].apply(stem_words)\n",
    "df['text_lemmas'] = df['text'].apply(lemmatize_words)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['text_lemmas']\n",
    "target = df['toxic']\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=123455)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (127656, 181158)\nMatrix shape: (31915, 181158)\nMatrix shape: (127656,)\nMatrix shape: (31915,)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer()\n",
    "tf_idf_train_feautures = count_tf_idf.fit_transform(features_train)\n",
    "tf_idf_test_feautures = count_tf_idf.transform(features_test)\n",
    "#print(tf_idf_train_feautures)\n",
    "#print(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", tf_idf_train_feautures.shape)\n",
    "print(\"Matrix shape:\", tf_idf_test_feautures.shape)\n",
    "print(\"Matrix shape:\", target_train.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF + ML approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (127656, 181158)\nMatrix shape: (31915, 181158)\nMatrix shape: (127656,)\nMatrix shape: (31915,)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer()\n",
    "tf_idf_train_feautures = count_tf_idf.fit_transform(features_train)\n",
    "tf_idf_test_feautures = count_tf_idf.transform(features_test)\n",
    "#print(tf_idf_train_feautures)\n",
    "#print(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", tf_idf_train_feautures.shape)\n",
    "print(\"Matrix shape:\", tf_idf_test_feautures.shape)\n",
    "print(\"Matrix shape:\", target_train.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (31915,)\nMatrix shape: (31915,)\nf1 log: 0.7485461091110496\nWall time: 761 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=12345,solver='liblinear',class_weight='balanced')\n",
    "model.fit(tf_idf_train_feautures, target_train)\n",
    "predictions = model.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1 = f1_score(target_test, predictions)\n",
    "print(\"f1 log:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {\n",
    "#    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "#    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "#    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "#}\n",
    "#modelsgd = SGDClassifier(max_iter=1000,shuffle=True,random_state=12345)\n",
    "#grid_search_sgd = GridSearchCV(modelsgd, param_grid=params)\n",
    "#grid_search_sgd.fit(tf_idf_train_feautures, target_train)\n",
    "#print(grid_search_sgd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (31915,)\nMatrix shape: (31915,)\nf1 sgd: 0.7732629727352682\nWall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
    "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='none',\n",
    "              power_t=0.5, random_state=12345, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "sgd.fit(tf_idf_train_feautures, target_train)\n",
    "predictions_sgd = sgd.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions_sgd.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_sgd = f1_score(target_test, predictions_sgd)\n",
    "print(\"f1 sgd:\",f1_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (31915,)\nMatrix shape: (31915,)\nf1 clf: 0.5835356433458398\nWall time: 10min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SVC(kernel='linear',C=0.05,random_state=12345)\n",
    "clf.fit(tf_idf_train_feautures, target_train)\n",
    "predictions_clf = clf.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions_clf.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_clf = f1_score(target_test, predictions_clf)\n",
    "print(\"f1 clf:\",f1_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (31915,)\nMatrix shape: (31915,)\nf1 knn: 0.056303549571603426\nWall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "knn.fit(tf_idf_train_feautures, target_train)\n",
    "predictions_knn = knn.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions_knn.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_knn = f1_score(target_test, predictions_knn)\n",
    "print(\"f1 knn:\",f1_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT + ML approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ones = df[df['toxic'] == 1].sample(10000, random_state=123)\n",
    "df_zeros = df[df['toxic'] == 0].sample(10000, random_state=123)\n",
    "new_df = shuffle(pd.concat([df_ones] + [df_zeros]))\n",
    "\n",
    "tokenized = new_df['text'].apply(lambda x: tokenizer.encode(x[:512], add_special_tokens=True))\n",
    "max_len = max(map(len, tokenized))\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a522d33c0b264715803cbdfdd381ad4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20 \n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.cuda.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
    "    attention_mask_batch = torch.cuda.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "    embeddings.append(batch_embeddings[0][:,0,:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bert = np.concatenate(embeddings)\n",
    "target_bert = new_df['toxic']\n",
    "features_train_bert, features_test_bert, target_train_bert, target_test_bert = train_test_split(features_bert, target_bert, test_size=0.2, random_state=123455)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 train: 0.8722\nF1 test: 0.8368\nWall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_lr = LogisticRegression(solver='liblinear', random_state=123455)\n",
    "model_lr.fit(features_train_bert, target_train_bert)\n",
    "\n",
    "pred_train = model_lr.predict(features_train_bert)\n",
    "pred_test = model_lr.predict(features_test_bert)\n",
    "\n",
    "f1_BERT = f1_score(target_test_bert, pred_test)\n",
    "\n",
    "print('F1 train: {:.4f}'.format(f1_score(target_train_bert, pred_train)))\n",
    "print('F1 test: {:.4f}'.format(f1_BERT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {\n",
    "#    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "#    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "#    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "#}\n",
    "#modelsgd = SGDClassifier(max_iter=100,shuffle=True,random_state=12345)\n",
    "#grid_search_sgd = GridSearchCV(modelsgd, param_grid=params)\n",
    "#grid_search_sgd.fit(features_train_bert, target_train_bert)\n",
    "#print(grid_search_sgd.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (4000,)\nMatrix shape: (4000,)\nf1 BERT SGD: 0.8325333333333333\nWall time: 260 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_bert = SGDClassifier(alpha=0.01, max_iter=100, random_state=12345)\n",
    "sgd_bert.fit(features_train_bert, target_train_bert)\n",
    "predictions_sgd = sgd_bert.predict(features_test_bert)\n",
    "print(\"Matrix shape:\", predictions_sgd.shape)\n",
    "print(\"Matrix shape:\", target_test_bert.shape)\n",
    "f1_sgd_BERT = f1_score(target_test_bert, predictions_sgd)\n",
    "print(\"f1 BERT SGD:\",f1_sgd_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "f1 TF-IDF Logistic: 0.7485461091110496\nf1 TF-IDF SGD: 0.7732629727352682\nf1 TF-IDF CLF: 0.5835356433458398\nf1 TF-IDF KNN: 0.056303549571603426\nf1 BERT Logistic: 0.8367816091954023\nf1 BERT SGD: 0.8325333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"f1 TF-IDF Logistic:\",f1)\n",
    "print(\"f1 TF-IDF SGD:\",f1_sgd)\n",
    "print(\"f1 TF-IDF CLF:\",f1_clf)\n",
    "print(\"f1 TF-IDF KNN:\",f1_knn)\n",
    "print(\"f1 BERT Logistic:\",f1_BERT)\n",
    "print(\"f1 BERT SGD:\",f1_sgd_BERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}